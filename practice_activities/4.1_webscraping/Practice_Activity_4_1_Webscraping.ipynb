{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrJX4FDa8oA8"
      },
      "source": [
        "# XML, HTML, and Web Scraping\n",
        "\n",
        "JSON and XML are two different ways to represent hierarchical data. Which one is better? There are lots of articles online which discuss similarities and differences between JSON and XML and their advantages and disadvantages. Both formats are still in current usage, so it is good to be familiar with both. However, JSON is more common, so we'll focus on working with JSON representations of hierarchical data.\n",
        "\n",
        "The reading covered an example of using Beautiful Soup to parse XML. Rather than doing another example XML now, we'll skip straight to scraping HTML from a webpage. Both HTML and XML can be parsed in a similar way with Beautiful Soup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XZhT8jhbuZSg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApqnMQ4iV4qu"
      },
      "source": [
        "## Scraping an HTML table with Beautiful Soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SD7XOs_So3G"
      },
      "source": [
        "Open the URL https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population and scroll down until you see a table of the cities in the U.S. with population over 100,000 (as of Jul 1, 2022). We'll use Beautiful Soup to scrape information from this table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRmnzgaQS_T0"
      },
      "source": [
        "Read in the HTML from the ULR using the `requests` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xvYzbSospYVu"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "import requests\n",
        "response = requests.get(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ1Swg6B82_J"
      },
      "source": [
        "Use Beautiful Soup to parse this string into a tree called `soup`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "e0jpmfwtpaEB"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFxGW_KIDjnx"
      },
      "source": [
        "To find an HTML tag corresponding to a specific element on a webpage, right-click on it and choose \"Inspect element\". Go to the cities table Wikipedia page and do this now.\n",
        "\n",
        "You should find that the cities table on the Wikipedia page corresponds to the element\n",
        "\n",
        "```\n",
        "<table class=\"wikitable sortable jquery-tablesorter\" style=\"text-align:center\">\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR50aTBZEwov"
      },
      "source": [
        "There are many `<table>` tags on the page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4691d-EGEwc0"
      },
      "outputs": [],
      "source": [
        "tables = soup.find_all(\"table\")\n",
        "\n",
        "#by looking at html i think its table 3 \n",
        "table = tables[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1xslM2yE1GI"
      },
      "source": [
        "We can use attributes like `class=` and `style=` to narrow down the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "E0Q0sa46DvTZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(soup.find_all(\"table\",\n",
        "                  attrs={\n",
        "                      \"class\": \"wikitable sortable\",\n",
        "                      \"style\": \"text-align:center\"}\n",
        "                  ))\n",
        "\n",
        "                  #code doesn't work\n",
        "\n",
        "                  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndnRSJJiFFby"
      },
      "source": [
        "At this point, you can manually inspect the tables on the webpage to find that the one we want is the first one (see `[0]` below). We'll store this as `table`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "sRBSqVGlYhuT"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m      2\u001b[0m                   attrs\u001b[38;5;241m=\u001b[39m{\n",
            "\u001b[1;32m      3\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitable sortable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m      4\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-align:center\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
            "\u001b[1;32m      5\u001b[0m                   )[\u001b[38;5;241m0\u001b[39m]\n",
            "\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "table = soup.find_all(\"table\",\n",
        "                  attrs={\n",
        "                      \"class\": \"wikitable sortable\",\n",
        "                      \"style\": \"text-align:center\"}\n",
        "                  )[0]\n",
        "\n",
        "                  #code doesn't work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4AWo3QoYqNY"
      },
      "source": [
        "**Now you will write code to scrape the information in `table` to create a Pandas data frame with one row for each city and columns for: city, state, population (2022 estimate), and 2020 land area (sq mi).** Refer to the Notes/suggestions below as you write your code. A few Hints are provided further down, but try coding first before looking at the hints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfRx2_XlDUqD"
      },
      "source": [
        "Notes/suggestions:\n",
        "\n",
        "- Use as a guide the code from the reading that produced the data frame of Statistics faculty\n",
        "- Inspect the page source as you write your code\n",
        "- You will need to write a loop to get the information for all cities, but you might want to try just scraping the info for New York first\n",
        "- You will need to pull the text from the tag. If `.text` returns text with \"\\n\" at the end, try `.get_text(strip = True)` instead of `.text`\n",
        "- Don't forget to convert to a Pandas Data Frame; it should have 333 rows and 4 columns\n",
        "- The goal of this exercise is just to create the Data Frame. If you were going to use it --- e.g., what is the population density for all cities in CA? --- then you would need to clean the data first (to clean strings and convert to quantitative). (You can use Beautiful Soup to do some of the cleaning for you, but that goes beyond our scope.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msKiUcOZpSX7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE. ADD AS MANY CELLS AS NEEDED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1M23w-PPtUyw"
      },
      "outputs": [],
      "source": [
        "#turn html into actual dataframe\n",
        "\n",
        "# initialize an empty list\n",
        "rows = []\n",
        "\n",
        "# iterate over all rows in the city table\n",
        "for city in table.find_all(\"tr\")[2:]:\n",
        "\n",
        "    # Get all the cells (<td>) in the row.\n",
        "    cells = city.find_all(\"td\")\n",
        "\n",
        "    # The information we need is the text between tags.\n",
        "\n",
        "    # Find the the name of the city in cell[0]\n",
        "    # which for most city is contained in the <a> tag\n",
        "    city_tag = cells[0].find(\"a\") or cells[0]\n",
        "    city = city_tag.text\n",
        "\n",
        "    # Find the office of the faculty in cell[1]\n",
        "    # which for most faculty is contained in the <a> tag\n",
        "    state_tag = cells[1].find(\"a\") or cells[1]\n",
        "    state = state_tag.text\n",
        "\n",
        "    # Find the email of the faculty in cell[3]\n",
        "    # which for most faculty is contained in the <a> tag\n",
        "    estimate_2023_tag = cells[2].find(\"td\") or cells[2]\n",
        "    estimate_2023 = estimate_2023_tag.text\n",
        "\n",
        "    census_2020_tag = cells[3].find(\"td\") or cells[3]\n",
        "    census_2020 = census_2020_tag.text\n",
        "\n",
        "    change_tag = cells[4].find(\"td\") or cells[4]\n",
        "    change = change_tag.text\n",
        "\n",
        "    land_area_miles_squared_tag = cells[5].find(\"td\") or cells[5]\n",
        "    land_area_miles_squared = land_area_miles_squared_tag.text\n",
        "\n",
        "    land_area_kilometer_squared_tag = cells[6].find(\"td\") or cells[6]\n",
        "    land_area_kilometer_squared = land_area_kilometer_squared_tag.text\n",
        "\n",
        "    density_miles_squared_tag = cells[7].find(\"td\") or cells[7]\n",
        "    density_miles_squared = density_miles_squared_tag.text\n",
        "\n",
        "    density_kilometer_squared_tag = cells[8].find(\"td\") or cells[8]\n",
        "    density_kilometer_squared = density_kilometer_squared_tag.text\n",
        "\n",
        "    location_tag = cells[9].find(\"small\") or cells[9]\n",
        "    location = location_tag.text\n",
        "\n",
        "    # Append this data.\n",
        "    rows.append({\n",
        "        \"city\": city,\n",
        "        \"state\": state,\n",
        "        \"2023 estimate\": estimate_2023,\n",
        "        \"2023 census\": census_2020,\n",
        "        \"Change\": change,\n",
        "        \"land area (miles^2)\": land_area_miles_squared,\n",
        "        \"landarea (kilometer^2)\": land_area_kilometer_squared,\n",
        "        \"density (miles^2)\": density_miles_squared,\n",
        "        \"density (kilometer^2)\": density_kilometer_squared,\n",
        "        \"location\": location\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>city</th>\n",
              "      <th>state</th>\n",
              "      <th>2023 estimate</th>\n",
              "      <th>2023 census</th>\n",
              "      <th>Change</th>\n",
              "      <th>land area (miles^2)</th>\n",
              "      <th>landarea (kilometer^2)</th>\n",
              "      <th>density (miles^2)</th>\n",
              "      <th>density (kilometer^2)</th>\n",
              "      <th>location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New York</td>\n",
              "      <td>NY</td>\n",
              "      <td>8,258,035\\n</td>\n",
              "      <td>8,804,190\\n</td>\n",
              "      <td>−6.20%\\n</td>\n",
              "      <td>300.5\\n</td>\n",
              "      <td>778.3\\n</td>\n",
              "      <td>29,298\\n</td>\n",
              "      <td>11,312\\n</td>\n",
              "      <td>40°40′N 73°56′W﻿ / ﻿40.66°N 73.94°W﻿ / 40.66; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>CA</td>\n",
              "      <td>3,820,914\\n</td>\n",
              "      <td>3,898,747\\n</td>\n",
              "      <td>−2.00%\\n</td>\n",
              "      <td>469.5\\n</td>\n",
              "      <td>1,216.0\\n</td>\n",
              "      <td>8,304\\n</td>\n",
              "      <td>3,206\\n</td>\n",
              "      <td>34°01′N 118°25′W﻿ / ﻿34.02°N 118.41°W﻿ / 34.02...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chicago</td>\n",
              "      <td>IL</td>\n",
              "      <td>2,664,452\\n</td>\n",
              "      <td>2,746,388\\n</td>\n",
              "      <td>−2.98%\\n</td>\n",
              "      <td>227.7\\n</td>\n",
              "      <td>589.7\\n</td>\n",
              "      <td>12,061\\n</td>\n",
              "      <td>4,657\\n</td>\n",
              "      <td>41°50′N 87°41′W﻿ / ﻿41.84°N 87.68°W﻿ / 41.84; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Houston</td>\n",
              "      <td>TX</td>\n",
              "      <td>2,314,157\\n</td>\n",
              "      <td>2,304,580\\n</td>\n",
              "      <td>+0.42%\\n</td>\n",
              "      <td>640.4\\n</td>\n",
              "      <td>1,658.6\\n</td>\n",
              "      <td>3,599\\n</td>\n",
              "      <td>1,390\\n</td>\n",
              "      <td>29°47′N 95°23′W﻿ / ﻿29.79°N 95.39°W﻿ / 29.79; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Phoenix</td>\n",
              "      <td>AZ</td>\n",
              "      <td>1,650,070\\n</td>\n",
              "      <td>1,608,139\\n</td>\n",
              "      <td>+2.61%\\n</td>\n",
              "      <td>518.0\\n</td>\n",
              "      <td>1,341.6\\n</td>\n",
              "      <td>3,105\\n</td>\n",
              "      <td>1,199\\n</td>\n",
              "      <td>33°34′N 112°05′W﻿ / ﻿33.57°N 112.09°W﻿ / 33.57...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>Yuma</td>\n",
              "      <td>AZ</td>\n",
              "      <td>100,858\\n</td>\n",
              "      <td>95,548\\n</td>\n",
              "      <td>+5.56%\\n</td>\n",
              "      <td>120.7\\n</td>\n",
              "      <td>312.6\\n</td>\n",
              "      <td>792\\n</td>\n",
              "      <td>306\\n</td>\n",
              "      <td>32°31′N 114°31′W﻿ / ﻿32.52°N 114.52°W﻿ / 32.52...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>New Bedford</td>\n",
              "      <td>MA</td>\n",
              "      <td>100,695\\n</td>\n",
              "      <td>101,079\\n</td>\n",
              "      <td>−0.38%\\n</td>\n",
              "      <td>20.0\\n</td>\n",
              "      <td>51.8\\n</td>\n",
              "      <td>5,054\\n</td>\n",
              "      <td>1,951\\n</td>\n",
              "      <td>41°40′N 70°56′W﻿ / ﻿41.66°N 70.94°W﻿ / 41.66; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>Suffolk</td>\n",
              "      <td>VA</td>\n",
              "      <td>100,659\\n</td>\n",
              "      <td>94,324\\n</td>\n",
              "      <td>+6.72%\\n</td>\n",
              "      <td>399.2\\n</td>\n",
              "      <td>1,033.9\\n</td>\n",
              "      <td>236\\n</td>\n",
              "      <td>91\\n</td>\n",
              "      <td>36°42′N 76°38′W﻿ / ﻿36.70°N 76.63°W﻿ / 36.70; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>Hesperia</td>\n",
              "      <td>CA</td>\n",
              "      <td>100,633\\n</td>\n",
              "      <td>99,818\\n</td>\n",
              "      <td>+0.82%\\n</td>\n",
              "      <td>72.7\\n</td>\n",
              "      <td>188.3\\n</td>\n",
              "      <td>1,373\\n</td>\n",
              "      <td>530\\n</td>\n",
              "      <td>34°24′N 117°19′W﻿ / ﻿34.40°N 117.32°W﻿ / 34.40...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>Davenport</td>\n",
              "      <td>IA</td>\n",
              "      <td>100,354\\n</td>\n",
              "      <td>101,724\\n</td>\n",
              "      <td>−1.35%\\n</td>\n",
              "      <td>63.8\\n</td>\n",
              "      <td>165.2\\n</td>\n",
              "      <td>1,594\\n</td>\n",
              "      <td>615\\n</td>\n",
              "      <td>41°34′N 90°36′W﻿ / ﻿41.56°N 90.60°W﻿ / 41.56; ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>336 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            city  ...                                           location\n",
              "0       New York  ...  40°40′N 73°56′W﻿ / ﻿40.66°N 73.94°W﻿ / 40.66; ...\n",
              "1    Los Angeles  ...  34°01′N 118°25′W﻿ / ﻿34.02°N 118.41°W﻿ / 34.02...\n",
              "2        Chicago  ...  41°50′N 87°41′W﻿ / ﻿41.84°N 87.68°W﻿ / 41.84; ...\n",
              "3        Houston  ...  29°47′N 95°23′W﻿ / ﻿29.79°N 95.39°W﻿ / 29.79; ...\n",
              "4        Phoenix  ...  33°34′N 112°05′W﻿ / ﻿33.57°N 112.09°W﻿ / 33.57...\n",
              "..           ...  ...                                                ...\n",
              "331         Yuma  ...  32°31′N 114°31′W﻿ / ﻿32.52°N 114.52°W﻿ / 32.52...\n",
              "332  New Bedford  ...  41°40′N 70°56′W﻿ / ﻿41.66°N 70.94°W﻿ / 41.66; ...\n",
              "333      Suffolk  ...  36°42′N 76°38′W﻿ / ﻿36.70°N 76.63°W﻿ / 36.70; ...\n",
              "334     Hesperia  ...  34°24′N 117°19′W﻿ / ﻿34.40°N 117.32°W﻿ / 34.40...\n",
              "335    Davenport  ...  41°34′N 90°36′W﻿ / ﻿41.56°N 90.60°W﻿ / 41.56; ...\n",
              "\n",
              "[336 rows x 10 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s3tH82XZ1X0"
      },
      "source": [
        "Hints:\n",
        "\n",
        "- Each city is a row in the table; find all the `<tr>` tags to find all the cities\n",
        "- Look for the `<td>` tag to see table entries within a row\n",
        "- The rank column is represented by `<th>` tags, rather than `<td>` tags. So within a row, the first (that is, `[0]`) `<td>` tag corresponds to the city name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIW4UgURNdhz"
      },
      "source": [
        "## Aside: Scraping an HTML table with Pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2ufAAMGYenH"
      },
      "source": [
        "The Pandas command `read_html` can be used to scrape information from an HTML table on a webpage.\n",
        "\n",
        "We can call `read_html` on the URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YnGD1hMbpv7H"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[                    Population tables of U.S. cities\n",
              " 0  The skyline of New York City, the most populou...\n",
              " 1                                             Cities\n",
              " 2  Population AreaDensityEthnic identityForeign-b...\n",
              " 3                                        Urban areas\n",
              " 4             Populous cities and metropolitan areas\n",
              " 5                                 Metropolitan areas\n",
              " 6  184 combined statistical areas935 core-based s...\n",
              " 7                                        Megaregions\n",
              " 8  See related population listsNorth American met...\n",
              " 9                                                vte,\n",
              "     0                               1\n",
              " 0 NaN                   State capital\n",
              " 1 NaN              State largest city\n",
              " 2 NaN  State capital and largest city\n",
              " 3 NaN                 Federal capital,\n",
              "             City  ST  ... 2020 density                               Location\n",
              "             City  ST  ...        / km2                               Location\n",
              " 0    New York[c]  NY  ...        11312    40°40′N 73°56′W﻿ / ﻿40.66°N 73.94°W\n",
              " 1    Los Angeles  CA  ...         3206  34°01′N 118°25′W﻿ / ﻿34.02°N 118.41°W\n",
              " 2        Chicago  IL  ...         4657    41°50′N 87°41′W﻿ / ﻿41.84°N 87.68°W\n",
              " 3        Houston  TX  ...         1390    29°47′N 95°23′W﻿ / ﻿29.79°N 95.39°W\n",
              " 4        Phoenix  AZ  ...         1199  33°34′N 112°05′W﻿ / ﻿33.57°N 112.09°W\n",
              " ..           ...  ..  ...          ...                                    ...\n",
              " 331         Yuma  AZ  ...          306  32°31′N 114°31′W﻿ / ﻿32.52°N 114.52°W\n",
              " 332  New Bedford  MA  ...         1951    41°40′N 70°56′W﻿ / ﻿41.66°N 70.94°W\n",
              " 333   Suffolk[l]  VA  ...           91    36°42′N 76°38′W﻿ / ﻿36.70°N 76.63°W\n",
              " 334     Hesperia  CA  ...          530  34°24′N 117°19′W﻿ / ﻿34.40°N 117.32°W\n",
              " 335    Davenport  IA  ...          615    41°34′N 90°36′W﻿ / ﻿41.56°N 90.60°W\n",
              " \n",
              " [336 rows x 10 columns],\n",
              "         Population  Number of municipal governments\n",
              " 0       1,000,000+                                9\n",
              " 1  700,000–999,999                               11\n",
              " 2  500,000–699,999                               18\n",
              " 3  400,000–499,999                               11\n",
              " 4  300,000–399,999                               20\n",
              " 5  200,000–299,999                               55\n",
              " 6  100,000–199,999                              212\n",
              " 7            Total                              336,\n",
              "                                                 State  Number of listed cities\n",
              " 0                                          California                       74\n",
              " 1                                               Texas                       42\n",
              " 2                                             Florida                       23\n",
              " 3                                             Arizona                       13\n",
              " 4                                            Colorado                       12\n",
              " 5                                      North Carolina                       10\n",
              " 6                           Massachusetts, Washington                        9\n",
              " 7                         Georgia, Illinois, Virginia                        8\n",
              " 8                                Michigan, New Jersey                        7\n",
              " 9   Missouri, Indiana, New York, Ohio, Oregon, Ten...                        6\n",
              " 10         Alabama, Connecticut, Kansas, Nevada, Utah                        5\n",
              " 11                                Louisiana, Oklahoma                        4\n",
              " 12  Iowa, Idaho, Minnesota, New Mexico, Pennsylvan...                        3\n",
              " 13                       Arkansas, Kentucky, Nebraska                        2\n",
              " 14  Alaska, District of Columbia, Hawaii, Maryland...                        1\n",
              " 15   Delaware, Maine, Vermont, West Virginia, Wyoming                        0,\n",
              "   Municipio  2023 estimate  ...  2020 density.1                             Location\n",
              " 0  San Juan         333005  ...       3,320/km2  18°24′N 66°04′W﻿ / ﻿18.40°N 66.06°W\n",
              " 1   Bayamón         180835  ...       2,648/km2  18°23′N 66°10′W﻿ / ﻿18.38°N 66.16°W\n",
              " 2  Carolina         150843  ...       2,888/km2  18°25′N 65°59′W﻿ / ﻿18.41°N 65.98°W\n",
              " 3     Ponce         130251  ...       1,869/km2  18°00′N 66°37′W﻿ / ﻿18.00°N 66.62°W\n",
              " 4    Caguas         124608  ...       4,507/km2  18°14′N 66°02′W﻿ / ﻿18.23°N 66.04°W\n",
              " \n",
              " [5 rows x 9 columns],\n",
              "    Census- designated place  ST  ... 2020 density                               Location\n",
              "    Census- designated place  ST  ...        / km2                               Location\n",
              " 0                 Arlington  VA  ...         3544    38°53′N 77°06′W﻿ / ﻿38.88°N 77.10°W\n",
              " 1                Enterprise  NV  ...         1298  36°01′N 115°14′W﻿ / ﻿36.01°N 115.23°W\n",
              " 2             Spring Valley  NV  ...         2345  36°06′N 115°16′W﻿ / ﻿36.10°N 115.26°W\n",
              " 3             Sunrise Manor  NV  ...         2356  36°11′N 115°03′W﻿ / ﻿36.18°N 115.05°W\n",
              " 4                  Paradise  NV  ...         1740  36°05′N 115°08′W﻿ / ﻿36.09°N 115.14°W\n",
              " 5                  Metairie  LA  ...         2378    30°00′N 90°11′W﻿ / ﻿30.00°N 90.18°W\n",
              " 6          East Los Angeles  CA  ...         6115  34°02′N 118°10′W﻿ / ﻿34.03°N 118.17°W\n",
              " 7                   Brandon  FL  ...         1337    27°56′N 82°18′W﻿ / ﻿27.94°N 82.30°W\n",
              " 8             The Woodlands  TX  ...         1020    30°10′N 95°31′W﻿ / ﻿30.17°N 95.51°W\n",
              " 9              Lehigh Acres  FL  ...          476    26°37′N 81°38′W﻿ / ﻿26.61°N 81.64°W\n",
              " 10              Spring Hill  FL  ...          732    28°29′N 82°32′W﻿ / ﻿28.48°N 82.53°W\n",
              " 11                Riverview  FL  ...          898    27°49′N 82°18′W﻿ / ﻿27.82°N 82.30°W\n",
              " 12                 Columbia  MD  ...         1267    39°12′N 76°52′W﻿ / ﻿39.20°N 76.86°W\n",
              " 13          Highlands Ranch  CO  ...         1644  39°32′N 104°58′W﻿ / ﻿39.54°N 104.97°W\n",
              " \n",
              " [14 rows x 10 columns],\n",
              "               City  ST  2023 estimate  ...  % decline from peak Peak year  Unnamed: 6\n",
              " 0        Allegheny  PA            NaN  ...                  NaN      1907         [z]\n",
              " 1         Brooklyn  NY            NaN  ...                  NaN      1898        [aa]\n",
              " 2           Camden  NJ        71100.0  ...              −42.92%      1950         NaN\n",
              " 3           Canton  OH        69197.0  ...              −40.81%      1950         NaN\n",
              " 4   Citrus Heights  CA        86239.0  ...              −19.73%      1990         NaN\n",
              " 5        Daly City  CA        99833.0  ...               −4.83%      2020         NaN\n",
              " 6           Duluth  MN        87680.0  ...              −18.29%      1960         NaN\n",
              " 7             Erie  PA        92957.0  ...              −32.85%      1960         NaN\n",
              " 8       Fall River  MA        93840.0  ...              −22.11%      1920         NaN\n",
              " 9      Federal Way  WA        97701.0  ...               −3.30%      2020         NaN\n",
              " 10           Flint  MI        79661.0  ...              −59.55%      1960         NaN\n",
              " 11            Gary  IN        67652.0  ...              −62.06%      1960         NaN\n",
              " 12         Hammond  IN        76193.0  ...              −31.79%      1960         NaN\n",
              " 13         Livonia  MI        92185.0  ...              −16.28%      1970         NaN\n",
              " 14   Niagara Falls  NY        47599.0  ...              −53.51%      1960         NaN\n",
              " 15         Norwalk  CA        98078.0  ...               −7.08%      2010         NaN\n",
              " 16           Parma  OH        78951.0  ...              −21.22%      1970         NaN\n",
              " 17      Portsmouth  VA        97454.0  ...              −15.09%      1960         NaN\n",
              " 18         Reading  PA        94903.0  ...              −14.63%      1930         NaN\n",
              " 19         Roanoke  VA        99578.0  ...               −0.64%      1980         NaN\n",
              " 20        Scranton  PA        75805.0  ...              −47.11%      1930         NaN\n",
              " 21      Somerville  MA        80407.0  ...              −22.62%      1930         NaN\n",
              " 22      St. Joseph  MO        70634.0  ...              −31.41%      1900        [ab]\n",
              " 23         Trenton  NJ        89620.0  ...              −29.99%      1950         NaN\n",
              " 24           Utica  NY        63607.0  ...              −37.48%      1930         NaN\n",
              " 25      Wilmington  DE        71675.0  ...              −36.29%      1940         NaN\n",
              " 26      Youngstown  OH        59108.0  ...              −65.23%      1930         NaN\n",
              " \n",
              " [27 rows x 7 columns],\n",
              "   vteThe 100 most populous cities of the United States  ...                                         Unnamed: 4\n",
              " 0  New York, New York Los Angeles, California Chi...    ...                                                NaN\n",
              " 1                                                NaN    ...  Chandler, Arizona North Las Vegas, Nevada Chul...\n",
              " 2  Cities ranked by United States Census Bureau p...    ...                                                NaN\n",
              " \n",
              " [3 rows x 5 columns],\n",
              "     0  ...                                                  4\n",
              " 0 NaN  ...  Chandler, Arizona North Las Vegas, Nevada Chul...\n",
              " \n",
              " [1 rows x 5 columns]]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQwWgh_cqynb"
      },
      "source": [
        "However, this scrapes all the tables on the webpage, not just the one we want. As with Beautiful Soup, we can narrow the search by specifying the table attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4BKvPxa9qJ2-"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "No tables found",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n",
            "\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_html(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\u001b[39m\u001b[38;5;124m\"\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwikitable sortable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-align:center\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:1240\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n",
            "\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n",
            "\u001b[1;32m   1225\u001b[0m     [\n",
            "\u001b[1;32m   1226\u001b[0m         is_file_like(io),\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m   1230\u001b[0m     ]\n",
            "\u001b[1;32m   1231\u001b[0m ):\n",
            "\u001b[1;32m   1232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n",
            "\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m   1237\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n",
            "\u001b[1;32m   1238\u001b[0m     )\n",
            "\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\n",
            "\u001b[1;32m   1241\u001b[0m     flavor\u001b[38;5;241m=\u001b[39mflavor,\n",
            "\u001b[1;32m   1242\u001b[0m     io\u001b[38;5;241m=\u001b[39mio,\n",
            "\u001b[1;32m   1243\u001b[0m     match\u001b[38;5;241m=\u001b[39mmatch,\n",
            "\u001b[1;32m   1244\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n",
            "\u001b[1;32m   1245\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n",
            "\u001b[1;32m   1246\u001b[0m     skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n",
            "\u001b[1;32m   1247\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n",
            "\u001b[1;32m   1248\u001b[0m     thousands\u001b[38;5;241m=\u001b[39mthousands,\n",
            "\u001b[1;32m   1249\u001b[0m     attrs\u001b[38;5;241m=\u001b[39mattrs,\n",
            "\u001b[1;32m   1250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n",
            "\u001b[1;32m   1251\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n",
            "\u001b[1;32m   1252\u001b[0m     converters\u001b[38;5;241m=\u001b[39mconverters,\n",
            "\u001b[1;32m   1253\u001b[0m     na_values\u001b[38;5;241m=\u001b[39mna_values,\n",
            "\u001b[1;32m   1254\u001b[0m     keep_default_na\u001b[38;5;241m=\u001b[39mkeep_default_na,\n",
            "\u001b[1;32m   1255\u001b[0m     displayed_only\u001b[38;5;241m=\u001b[39mdisplayed_only,\n",
            "\u001b[1;32m   1256\u001b[0m     extract_links\u001b[38;5;241m=\u001b[39mextract_links,\n",
            "\u001b[1;32m   1257\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n",
            "\u001b[1;32m   1258\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n",
            "\u001b[1;32m   1259\u001b[0m )\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:1003\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n",
            "\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
            "\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n",
            "\u001b[1;32m   1005\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:983\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n",
            "\u001b[1;32m    972\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n",
            "\u001b[1;32m    973\u001b[0m     io,\n",
            "\u001b[1;32m    974\u001b[0m     compiled_match,\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m    979\u001b[0m     storage_options,\n",
            "\u001b[1;32m    980\u001b[0m )\n",
            "\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;32m--> 983\u001b[0m     tables \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mparse_tables()\n",
            "\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n",
            "\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n",
            "\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n",
            "\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
            "\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n",
            "\u001b[1;32m    244\u001b[0m \n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n",
            "\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[0;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_doc(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n",
            "\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/html.py:598\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[0;34m(self, document, match, attrs)\u001b[0m\n",
            "\u001b[1;32m    596\u001b[0m tables \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39mfind_all(element_name, attrs\u001b[38;5;241m=\u001b[39mattrs)\n",
            "\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tables:\n",
            "\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo tables found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m    600\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[1;32m    601\u001b[0m unique_tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "\n",
            "\u001b[0;31mValueError\u001b[0m: No tables found"
          ]
        }
      ],
      "source": [
        "pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\", attrs = {'class': 'wikitable sortable', \"style\": \"text-align:center\"})\n",
        "\n",
        "#the filtering doesn't work but can use the 3 table in wiki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6P7xCnPrBtS"
      },
      "source": [
        "This still returns 3 tables. As we remarked above, the table that we want is the first one (see `[0]` below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96L5qJvGp7ks"
      },
      "outputs": [],
      "source": [
        "df_cities2 = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\")[2]\n",
        "df_cities2\n",
        "\n",
        "#the filtering doesn't work but can use the 3 table in wiki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjeczIIMYeqj"
      },
      "source": [
        "Wait, that seemed much easier than using Beautiful Soup, and it returned a data frame, and we even got for free some formatting like removing the commas from the population! Why didn't we just use `read_html` in the first place? It's true the `read_html` works well when scraping information from an HTML *table*. Unfortunately, you often want to scrape information from a webpage that isn't conveniently stored in an HTML table, in which case `read_html` won't work. (It only searches for `<table>`, `<th>`, `<tr>`, and `<td>` tags, but there are many other HTML tags.) Though Beautiful Soup is not as simple as `read_html`, it is more flexible and thus more widely applicable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctj79YpgX6hw"
      },
      "source": [
        "## Scraping information that is NOT in a `<table>` with Beautiful Soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK6rJQbuuWwF"
      },
      "source": [
        "The Cal Poly course catalog http://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory contains a list of courses offered by the Statistics department. **You will scrape this website to obtain a Pandas data frame with one row for each DATA or STAT course and two columns: course name and number (e.g, DATA 301. Introduction to Data Science) and term typically offered (e.g., Term Typically Offered: F, W, SP).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbLLrwxs0eWd"
      },
      "source": [
        "Note: Pandas `read_html` is not help here since the courses are not stored in a `<table>.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIRewkca0jhz"
      },
      "outputs": [],
      "source": [
        "pd.read_html(\"http://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvSrhxS4Se7a"
      },
      "source": [
        "\n",
        "Notes/suggestions:\n",
        "\n",
        "\n",
        "- Inspect the page source as you write your code\n",
        "- The courses are not stored in a `<table>`. How are they stored?\n",
        "- You will need to write a loop to get the information for all courses, but you might want to try just scraping the info for DATA 100 first\n",
        "- What kind of tag is the course name stored in? What is the `class` of the tag?\n",
        "- What kind of tag is the quarter(s) the course is offered stored in? What is the `class` of the tag? Is this the only tag of this type with the class? How will you get the one you want?\n",
        "- You don't have to remove the number of units (e.g., 4 units) from the course name and number, but you can try it if you want\n",
        "- You will need to pull the text from the tag. If `.text` returns text with \"\\n\" at the end, try `get_text(strip = True)` instead of `text`\n",
        "- Don't forget to convert to a Pandas Data Frame; it should have 74 rows and 2 columns\n",
        "- The goal of this exercise is just to create the Data Frame. If you were going to use it then you might need to clean the data first. (You can use Beautiful Soup to do some of the cleaning for you, but that goes beyond our scope.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "import requests\n",
        "response = requests.get(\"http://catalog.calpoly.edu/collegesandprograms/collegeofsciencemathematics/statistics/#courseinventory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "page = soup.find_all(\"div\",\n",
        "                  attrs={\n",
        "                      \"id\": \"courseinventorycontainer\"}\n",
        "                  )\n",
        "\n",
        "len(page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "page = soup.find_all(\"div\", {\"class\": \"courseblock\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "EW2sWIGavIFc"
      },
      "outputs": [],
      "source": [
        "#turn html into actual dataframe\n",
        "\n",
        "# initialize an empty list\n",
        "rows = []\n",
        "\n",
        "# iterate over all vals in the courses\n",
        "#each course is seperated by div\n",
        "for courseblock in page:\n",
        "\n",
        "    # course_title\n",
        "    course_tag = courseblock.find(\"p\", {\"class\": \"courseblocktitle\"})\n",
        "    course = course_tag.get_text().strip()\n",
        "\n",
        "    #term offered\n",
        "    term_tag = courseblock.find(\"div\", {\"class\": \"noindent courseextendedwrap\"})\n",
        "    term = term_tag.get_text().strip()\n",
        "    # Append this data.\n",
        "    rows.append({\n",
        "        \"course\": course,\n",
        "        \"term offered\": term\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>course</th>\n",
              "      <th>term offered</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DATA 100. Data Science for All I.</td>\n",
              "      <td>Term Typically Offered: F, W, SP2020-21 or lat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DATA 301. Introduction to Data Science.</td>\n",
              "      <td>Term Typically Offered: F, W, SPPrerequisite: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DATA 401. Data Science Process and Ethics.</td>\n",
              "      <td>Term Typically Offered: FPrerequisites: DATA 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DATA 402. Mathematical Foundations of Data Sci...</td>\n",
              "      <td>Term Typically Offered: FPrerequisites: CSC 46...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DATA 403. Data Science Projects Laboratory.</td>\n",
              "      <td>Term Typically Offered: FConcurrent: DATA 401 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>STAT 551. Statistical Learning with R.</td>\n",
              "      <td>Term Typically Offered: FPrerequisite: STAT 30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>STAT 566. Graduate Consulting Practicum.</td>\n",
              "      <td>Term Typically Offered: SPPrerequisite: STAT 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>STAT 570. Selected Advanced Topics.</td>\n",
              "      <td>Term Typically Offered: TBDPrerequisite: Gradu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>STAT 590. Graduate Seminar in Statistics.</td>\n",
              "      <td>Term Typically Offered: F, W, SPPrerequisite: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>STAT 599. Thesis.</td>\n",
              "      <td>Term Typically Offered: TBDPrerequisite: Gradu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>74 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               course                                       term offered\n",
              "0                   DATA 100. Data Science for All I.  Term Typically Offered: F, W, SP2020-21 or lat...\n",
              "1             DATA 301. Introduction to Data Science.  Term Typically Offered: F, W, SPPrerequisite: ...\n",
              "2          DATA 401. Data Science Process and Ethics.  Term Typically Offered: FPrerequisites: DATA 3...\n",
              "3   DATA 402. Mathematical Foundations of Data Sci...  Term Typically Offered: FPrerequisites: CSC 46...\n",
              "4         DATA 403. Data Science Projects Laboratory.  Term Typically Offered: FConcurrent: DATA 401 ...\n",
              "..                                                ...                                                ...\n",
              "69             STAT 551. Statistical Learning with R.  Term Typically Offered: FPrerequisite: STAT 30...\n",
              "70           STAT 566. Graduate Consulting Practicum.  Term Typically Offered: SPPrerequisite: STAT 4...\n",
              "71                STAT 570. Selected Advanced Topics.  Term Typically Offered: TBDPrerequisite: Gradu...\n",
              "72          STAT 590. Graduate Seminar in Statistics.  Term Typically Offered: F, W, SPPrerequisite: ...\n",
              "73                                  STAT 599. Thesis.  Term Typically Offered: TBDPrerequisite: Gradu...\n",
              "\n",
              "[74 rows x 2 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17e8M_OsaHJz"
      },
      "source": [
        "Hints:\n",
        "\n",
        "- Each course is represented by a `<div>` with `class=courseblock`, so you can find all the courses with `soup.find_all(\"div\", {\"class\": \"courseblock\"})`\n",
        "- The course name is in a `<p>` tag with `class=courseblocktitle`, inside a `<strong>` tag. (Though I don't think we need to find the strong tag here.)\n",
        "- The term typically offered is in `<p>` tag with `class=noindent`. However, there are several tags with this class; term typically offered is the first one.\n",
        "- If you want to use Beautiful Soup to remove the course units (e.g., 4 units), find the `<span>` tag within the course name tag and `.extract()` this span tag"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
